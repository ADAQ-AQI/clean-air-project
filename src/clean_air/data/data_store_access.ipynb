{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Clean Air Framework Datastore\n",
    "Data is stored on an object store on the JASMIN cloud as a \"dataset\".\n",
    "A dataset is composed of all the datafiles that hold the data (e.g. netcdf or CSV files) plus metadata about the dataset.\n",
    "Metadata can be accessed independently of the data, so that you don't have to download the entire dataset if you don't need it.\n",
    "\n",
    "Data objects are provided to represent the stored data.\n",
    "Datasets are encapsulated by the `DataSet` class, whilst metadata is encapsulated by the `Metadata` class.\n",
    "Access to stored datasets and metadata are via the `S3FSDataSetStore` and `S3FSMetdataStore` classes respectively.\n",
    "(`s3fs` is the underlying library used to access the object store)\n",
    "\n",
    "Both data access classes provide a similar interface, working with their respective data objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Credentials for Object Store Access\n",
    "Anonymous access is enabled for the data bucket, but to be able to upload data credentials with write permissions are \n",
    "required.\n",
    "\n",
    "Documentation on `s3fs`'s credentials are here: https://s3fs.readthedocs.io/en/latest/#credentials\n",
    "As it uses Amazon AWS's `botocore` library, the credential config is the same as for `botocore` & `boto3`.\n",
    "\n",
    "In summary, the options are (in order of increasing precedence):\n",
    "\n",
    "* `~/.aws/credentials` file (refer to [Configuration and credential file settings](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)) \n",
    "with contents such as:\n",
    "```\n",
    "[default]\n",
    "aws_access_key_id=AKIAIOSFODNN7EXAMPLE\n",
    "aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n",
    "```\n",
    "* Setting the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables\n",
    "* Explicitly creating an `s3fs.S3FileSystem` instance and passing them as arguments (refer to next section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating a DataSetStore or MetadataStore\n",
    "### Helper Functions\n",
    "Helper functions are available that create instances of both `S3FSDataSetStore` and `S3FSMetadataStore`:\n",
    "* `create_dataset_store`\n",
    "* `create_metadata_store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from clean_air.data.storage import create_dataset_store, create_metadata_store\n",
    "\n",
    "TEST_BUCKET_NAME = \"test-data\"\n",
    "dataset_store = create_dataset_store(storage_bucket_name=TEST_BUCKET_NAME)\n",
    "metadata_store = create_metadata_store(storage_bucket_name=TEST_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions support the same parameters that are used to customise the resulting instances:\n",
    "\n",
    "| Parameter Name        | Description | Default |\n",
    "| --------------------- | ----------- | ------- |\n",
    "| `storage_bucket_name` | Name of the bucket where datasets are stored | `caf-data` |\n",
    "| `local_storage_path`  | Path to a writeable directory to store local copies of dataset files | A temporary directory in the systems `tmp` folder |\n",
    "| `endpoint_url`        | the object store service endpoint URL. Changes depending on whether accessing data from inside or outside JASMIN, or using data stored on another AWS S3 compatible object store | `clean_air.data.storage.JasminEndpointUrls.EXTERNAL` |\n",
    "| `anon`                | Whether to use anonymous access or credentials. `anon=False` is required for write access | `True` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Advanced options\n",
    "The helper function creates a `DataSetStore` in its most common configurations, but for more control you can create an \n",
    "instance by using the constructor directly and providing the required arguments.\n",
    "\n",
    "Here's an example of how to pass in credentials programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "from clean_air.data.storage import JasminEndpointUrls\n",
    "from clean_air.data.storage import S3FSDataSetStore, S3FSMetadataStore\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=\"AKIAIOSFODNN7EXAMPLE\",\n",
    "    secret=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n",
    "    client_kwargs={\"endpoint_url\": JasminEndpointUrls.EXTERNAL}\n",
    ")\n",
    "\n",
    "metadata_store_with_custom_credentials = S3FSMetadataStore(fs, storage_bucket_name=TEST_BUCKET_NAME)\n",
    "dataset_store_with_custom_credentials = S3FSDataSetStore(fs, metadata_store=metadata_store_with_custom_credentials,\n",
    "                                                         storage_bucket_name=TEST_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from clean_air.models import Metadata, DataSet\n",
    "\n",
    "# Credentials will need to be configured correctly for this to work\n",
    "dataset_store_with_write_access = create_dataset_store(TEST_BUCKET_NAME, anon=False)\n",
    "\n",
    "# Create new dataset\n",
    "with tempfile.TemporaryDirectory() as data_dir_path:\n",
    "    # Create some test data\n",
    "    test_datafile = Path(data_dir_path + \"/testfile.txt\")\n",
    "    test_datafile.touch()\n",
    "    metadata = Metadata(dataset_name=\"TestDataSet\",\n",
    "                        extent=Polygon([[-1, -1], [1, -1], [1, 1], [-1, 1]]))\n",
    "    test_dataset = DataSet(files=[test_datafile], metadata=metadata)\n",
    "\n",
    "    # Upload it\n",
    "    dataset_store_with_write_access.put(test_dataset)\n",
    "\n",
    "metadata_store_with_write_access = create_metadata_store(TEST_BUCKET_NAME, anon=False)\n",
    "# Update the metadata...\n",
    "metadata.description = \"This is a test data set\"\n",
    "# ...and upload it\n",
    "metadata_store_with_write_access.put(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Discovering Data\n",
    "Both `S3FSDataSetStore` and `S3FSMetadataStore` both have an `available_datasets` method to discover what datasets exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_store.available_datasets = ['TestDataSet', 'testdataset']\n",
      "metadata_store.available_datasets = ['TestDataSet', 'testdataset']\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset_store.available_datasets = {dataset_store.available_datasets()}\")\n",
    "print(f\"metadata_store.available_datasets = {metadata_store.available_datasets()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Downloading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet(files=[PosixPath('/var/tmp/tmpe14wm5eo/testdataset/testfile.txt')], metadata=Metadata(dataset_name='TestDataSet', extent=<shapely.geometry.polygon.Polygon object at 0x7f4f032ad910>, crs=<Geographic 2D CRS: EPSG:4326>\n",
      "Name: WGS 84\n",
      "Axis Info [ellipsoidal]:\n",
      "- Lat[north]: Geodetic latitude (degree)\n",
      "- Lon[east]: Geodetic longitude (degree)\n",
      "Area of Use:\n",
      "- name: World.\n",
      "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
      "Datum: World Geodetic System 1984 ensemble\n",
      "- Ellipsoid: WGS 84\n",
      "- Prime Meridian: Greenwich\n",
      ", description='This is a test data set', data_type=<DataType.OTHER: 'other'>, contacts=[]))\n",
      "Metadata(dataset_name='TestDataSet', extent=<shapely.geometry.polygon.Polygon object at 0x7f4f032ad400>, crs=<Geographic 2D CRS: EPSG:4326>\n",
      "Name: WGS 84\n",
      "Axis Info [ellipsoidal]:\n",
      "- Lat[north]: Geodetic latitude (degree)\n",
      "- Lon[east]: Geodetic longitude (degree)\n",
      "Area of Use:\n",
      "- name: World.\n",
      "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
      "Datum: World Geodetic System 1984 ensemble\n",
      "- Ellipsoid: WGS 84\n",
      "- Prime Meridian: Greenwich\n",
      ", description='This is a test data set', data_type=<DataType.OTHER: 'other'>, contacts=[])\n"
     ]
    }
   ],
   "source": [
    "test_dataset_id = \"testdataset\"\n",
    "\n",
    "dataset = dataset_store.get(test_dataset_id)\n",
    "print(str(dataset))\n",
    "\n",
    "metadata = metadata_store.get(test_dataset_id)\n",
    "print(str(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deployment\n",
    "For read-only/anonmous access to work correctly, the data storage bucket must have this policy applied:\n",
    "```json\n",
    "{\"Version\": \"2008-10-17\",\n",
    " \"Id\": \"Read Access For Anonymous Users\",\n",
    " \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"Read-only and list bucket access for Everyone\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\"anonymous\": [\"*\"]},\n",
    "      \"Resource\": \"*\",\n",
    "      \"Action\": [\"GetObject\", \"ListBucket\"]\n",
    "    }\n",
    "]\n",
    "}\n",
    "```\n",
    "Note, this policy was written for and tested with the JASMIN object store, so may need tweaking for use with\n",
    "other object stores."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Developer Note: s3fs vs s3fs-fuse\n",
    "\n",
    "[s3fs](https://pypi.org/project/s3fs/) is a python library that provides a filesystem-like interface,\n",
    "but actually just uses the S3 APIs under the hood and doesn't interact with the local filesystem\n",
    "\n",
    "[s3fs-fuse](https://github.com/s3fs-fuse/s3fs-fuse) actually mounts an S3 compatible bucket as a virtual filesystem,\n",
    "which can be accessed using any method that works for local files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}